<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>INFM109: Large Language Models</title>
    <link
      rel="icon"
      type="image/png"
      sizes="32x32"
      href="./../assets/images/favicon-32x32.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="16x16"
      href="./../assets/images/favicon-16x16.png"
    />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Arimo:ital,wght@0,400;0,700;1,400;1,700&family=Roboto:ital,wght@0,400;0,700;1,400;1,700&display=swap"
      rel="stylesheet"
    />
    <script src="./../assets/scripts/code_snippets.js" defer></script>
    <script src="./../assets/scripts/focus_content.js" defer></script>
    <script src="./../assets/scripts/table_of_contents.js" defer></script>

    <!-- Bootstrap / Sass -->
    <link rel="stylesheet" href="./../assets/css/styles.css" />
    <link rel="stylesheet" href="./../assets/css/bootstrap-icons.css" />
    <script src="./../assets/scripts/bootstrap.bundle.min.js" defer></script>
  </head>
  <body class="bg-light">
    <div class="content-wrapper">
      <div class="toc">
        <ul>
          <li>
            <a href="#large-language-models">Large Language Models</a>
            <ul>
              <li>
                <a href="#what-is-an-llm">What is an LLM?</a>
                <ul>
                  <li>
                    <a href="#llms-are-not-databases">LLMs Are Not Databases</a>
                  </li>
                </ul>
              </li>
              <li>
                <a href="#key-concepts-vocabulary">Key Concepts Vocabulary</a>
                <ul>
                  <li><a href="#tokenization">Tokenization</a></li>
                  <li><a href="#context-window">Context Window</a></li>
                  <li><a href="#the-system-prompt">The System Prompt</a></li>
                  <li><a href="#knowledge-cutoffs">Knowledge Cutoffs</a></li>
                </ul>
              </li>
              <li>
                <a href="#training-process">Training Process</a>
                <ul>
                  <li><a href="#pre-training">Pre-training</a></li>
                  <li>
                    <a href="#post-training-fine-tuning"
                      >Post-training (Fine-tuning)</a
                    >
                  </li>
                  <li>
                    <a href="#safety-considerations">Safety Considerations</a>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </div>

      <h1 id="large-language-models">Large Language Models</h1>
      <h2 id="what-is-an-llm">What is an LLM?</h2>
      <figure>
        <span>
          <img
            src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/ef/ChatGPT-Logo.svg/500px-ChatGPT-Logo.svg.png"
            style="width: 30%; height: auto"
          />
        </span>
      </figure>

      <p>
        <strong>Large Language Models (LLMs)</strong> are a kind of AI that are
        trained to predict the next word in a sequence of text.
      </p>
      <p>
        They are the technology behind tools like ChatGPT, Claude, and Google
        Gemini.
      </p>
      <div class="focusContent note">
        <p>
          <strong>Key Insight</strong>: LLMs are fundamentally prediction
          engines, not databases or search engines.
        </p>
        <p>Think of it like an extremely sophisticated autocomplete:</p>
        <ul>
          <li>You type: &quot;The weather today is...&quot;</li>
          <li>Your phone suggests: &quot;nice&quot; or &quot;sunny&quot;</li>
          <li>
            An LLM can continue: &quot;quite pleasant with temperatures reaching
            72Â°F and light winds from the southwest&quot;
          </li>
        </ul>
      </div>

      <h3 id="llms-are-not-databases">LLMs Are Not Databases</h3>
      <p>This is crucial to understand:</p>
      <table>
        <thead>
          <tr>
            <th align="left">Database</th>
            <th align="left">LLM</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left">Stores exact information</td>
            <td align="left">Generates probable responses</td>
          </tr>
          <tr>
            <td align="left">Retrieves facts precisely</td>
            <td align="left">Predicts likely continuations</td>
          </tr>
          <tr>
            <td align="left">Always consistent</td>
            <td align="left">May vary between responses</td>
          </tr>
          <tr>
            <td align="left">Can cite sources</td>
            <td align="left">May &quot;hallucinate&quot; information</td>
          </tr>
        </tbody>
      </table>
      <p>
        <strong>Hallucination</strong>: When an LLM generates information that
        sounds plausible but is factually incorrect.
      </p>
      <h2 id="key-concepts-vocabulary">Key Concepts Vocabulary</h2>
      <h3 id="tokenization">Tokenization</h3>
      <p>
        <strong>Token</strong>: The basic unit of text that an LLM processes -
        roughly equivalent to words, but not exactly.
      </p>
      <div class="focusContent exercise">
        <p>
          Open up the following link and try out a few sentences:
          <a href="https://tiktokenizer.vercel.app/"
            >https://tiktokenizer.vercel.app/</a
          >
        </p>
        <p>You&#39;ll see that:</p>
        <ul>
          <li>Each meaningful part of a word is a token</li>
        </ul>
        <figure>
          <span>
            <img src="images/tokens1.png" style="width: 80%; height: auto" />
          </span>
        </figure>

        <figure>
          <span>
            <img src="images/tokens2.png" style="width: 80%; height: auto" />
          </span>
        </figure>

        <ul>
          <li>
            The model knows who is speaking by wrapping the actual messages in
            special tokens (e.g. <code>&lt;|user|&gt;</code> and
            <code>&lt;|assistant|&gt;</code>)
          </li>
        </ul>
        <figure>
          <span>
            <img src="images/tokens3.png" style="width: 80%; height: auto" />
          </span>
        </figure>
      </div>

      <h3 id="context-window">Context Window</h3>
      <figure>
        <span>
          <img
            src="images/context_window.svg"
            style="width: 100%; height: auto"
          />
        </span>
      </figure>

      <p>
        The <strong>context window</strong> is the amount of text an LLM can
        &quot;remember&quot; and work with at one time - like the model&#39;s
        working memory.
      </p>
      <p>
        It operates like a sliding window that moves through the text as the
        model generates responses.
      </p>
      <p><strong>Current Context Window Sizes</strong>:</p>
      <table>
        <thead>
          <tr>
            <th>Model</th>
            <th>Developer</th>
            <th>Context Window Size</th>
            <th>Notes</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Claude 4 Opus</td>
            <td>Anthropic</td>
            <td>200,000 tokens</td>
            <td>Latest flagship model</td>
          </tr>
          <tr>
            <td>Claude 4 Sonnet</td>
            <td>Anthropic</td>
            <td>200,000 tokens</td>
            <td>Current model</td>
          </tr>
          <tr>
            <td>Claude 3.5 Sonnet</td>
            <td>Anthropic</td>
            <td>200,000 tokens</td>
            <td>Previous generation</td>
          </tr>
          <tr>
            <td>GPT-4 Turbo</td>
            <td>OpenAI</td>
            <td>128,000 tokens</td>
            <td>Latest GPT-4 variant</td>
          </tr>
          <tr>
            <td>GPT-4o</td>
            <td>OpenAI</td>
            <td>128,000 tokens</td>
            <td>Multimodal model</td>
          </tr>
          <tr>
            <td>Gemini 1.5 Pro</td>
            <td>Google</td>
            <td>2,000,000 tokens</td>
            <td>Exceptionally large context</td>
          </tr>
          <tr>
            <td>Gemini 1.5 Flash</td>
            <td>Google</td>
            <td>1,000,000 tokens</td>
            <td>Faster variant</td>
          </tr>
          <tr>
            <td>Llama 3.1 405B</td>
            <td>Meta</td>
            <td>128,000 tokens</td>
            <td>Open source flagship</td>
          </tr>
          <tr>
            <td>Llama 3.1 70B</td>
            <td>Meta</td>
            <td>128,000 tokens</td>
            <td>Open source alternative</td>
          </tr>
          <tr>
            <td>Mixtral 8x22B</td>
            <td>Mistral AI</td>
            <td>65,000 tokens</td>
            <td>Mixture of experts</td>
          </tr>
        </tbody>
      </table>
      <p><strong>Why This Matters</strong>:</p>
      <ul>
        <li>
          The model can only reference information within its current context
        </li>
        <li>Larger context windows allow for more complex tasks</li>
        <li>
          The more context the model is using, the more computationally
          expensive it is to generate a response
        </li>
      </ul>
      <h3 id="the-system-prompt">The System Prompt</h3>
      <p>
        The <strong>system prompt</strong> is a special prompt that guides the
        assistant&#39;s behavior. This is typically used to start any
        conversation with a chatbot.
      </p>
      <p>
        Anthropic&#39;s system prompts for Claude may be found here:
        <a href="https://docs.anthropic.com/en/release-notes/system-prompts"
          >Claude System Prompts</a
        >. Note how this has changed between the first version and the latest!
      </p>
      <div class="focusContent demo">
        <p>
          The system prompt applies to the web interface for a given model, but
          there are ways to customize the overall tone and behavior of the
          model.
        </p>
        <p>
          Let&#39;s build a custom chatbot:
          <a
            href="https://github.com/mpjovanovich-IvyTechDemos/misc/blob/main/chatbot.py"
            >chatbot.py</a
          >
        </p>
      </div>

      <h3 id="knowledge-cutoffs">Knowledge Cutoffs</h3>
      <p>
        <strong>Knowledge cutoff</strong>: The date after which the LLM has no
        information about world events.
      </p>
      <p>For example:</p>
      <ul>
        <li>GPT-4: October 2023</li>
      </ul>
      <p><strong>Why This Matters</strong>:</p>
      <ul>
        <li>LLMs can&#39;t tell you about recent events (without tool use)</li>
        <li>Information about rapidly changing topics may be outdated</li>
        <li>Always verify recent information from current sources</li>
      </ul>
      <h2 id="training-process">Training Process</h2>
      <p>LLMs are created through a multi-stage training process:</p>
      <h3 id="pre-training">Pre-training</h3>
      <p><strong>Goal</strong>: Teach the model to predict the next word</p>
      <p><strong>Process</strong>:</p>
      <ol>
        <li>
          Gather massive amounts of text from the internet (books, articles,
          websites)
        </li>
        <li>Train the neural network to predict the next word in sequences</li>
        <li>
          This creates a &quot;base model&quot; that understands language
          patterns
        </li>
      </ol>
      <p><strong>Training Data Sources</strong>:</p>
      <ul>
        <li>Web pages and articles</li>
        <li>Books and literature</li>
        <li>Academic papers</li>
        <li>Reference materials</li>
        <li>Code repositories</li>
      </ul>
      <p>
        <strong>Result</strong>: A model that can complete text but may not be
        helpful or safe for conversations.
      </p>
      <p>
        The models from pre-training are not released to the public. These are
        not the finished product.
      </p>
      <h3 id="post-training-fine-tuning">Post-training (Fine-tuning)</h3>
      <p>
        <strong>Goal</strong>: Make the model helpful, harmless, and honest.
        Focus it on a specific task.
      </p>
      <p><strong>Key Techniques</strong>:</p>
      <p><strong>Supervised Fine-tuning</strong>:</p>
      <ul>
        <li>Human trainers create example conversations</li>
        <li>Model learns to follow instructions and be helpful</li>
      </ul>
      <p><strong>Reinforcement Learning from Human Feedback (RLHF)</strong>:</p>
      <ul>
        <li>Human raters rank different model responses</li>
        <li>Model learns to generate responses humans prefer</li>
        <li>Helps align the model with human values</li>
      </ul>
      <p>
        <strong>Result</strong>: A conversational AI that can follow
        instructions, admit uncertainty, and refuse harmful requests.
      </p>
      <h3 id="safety-considerations">Safety Considerations</h3>
      <p>
        A final step prior to releasing models to the public is to ensure the
        model is safe to use.
      </p>
      <p>
        Currently, the greatest danger to the public is not rogue AI behavior,
        but humans using the technology to do bad things. This may change as
        models are given more freedom and capabilities.
      </p>
      <div class="focusContent note">
        <p>
          Antropic has published a report of their safety studies on the model.
          The report shows how early and unfiltered versions of the model behave
          in potentially problematic scenarios, and highlights how the AI think
          in ways that are much different from human beings:
        </p>
        <p>
          <a
            href="https://www-cdn.anthropic.com/4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf"
            >Claude 4 System Card</a
          >
        </p>
      </div>
    </div>
  </body>
</html>
